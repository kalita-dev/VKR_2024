{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":120,"sourceType":"datasetVersion","datasetId":55},{"sourceId":5094410,"sourceType":"datasetVersion","datasetId":2958426},{"sourceId":56499,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":47433}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv('/kaggle/input/enron-email-dataset/emails.csv')","metadata":{"execution":{"iopub.status.busy":"2024-05-27T06:52:05.002301Z","iopub.execute_input":"2024-05-27T06:52:05.002628Z","iopub.status.idle":"2024-05-27T06:52:28.867977Z","shell.execute_reply.started":"2024-05-27T06:52:05.002603Z","shell.execute_reply":"2024-05-27T06:52:28.867108Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def get_text_from_email(msg):\n    '''To get the content from email objects'''\n    parts = []\n    for part in msg.walk():\n        if part.get_content_type() == 'text/plain':\n            parts.append( part.get_payload() )\n    return ''.join(parts)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T06:53:30.545419Z","iopub.execute_input":"2024-05-27T06:53:30.545736Z","iopub.status.idle":"2024-05-27T06:53:30.551953Z","shell.execute_reply.started":"2024-05-27T06:53:30.545710Z","shell.execute_reply":"2024-05-27T06:53:30.550977Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import sys, email\ndf['message'] = list(map(get_text_from_email, list(map(email.message_from_string, df['message']))))","metadata":{"execution":{"iopub.status.busy":"2024-05-27T06:53:31.770190Z","iopub.execute_input":"2024-05-27T06:53:31.770474Z","iopub.status.idle":"2024-05-27T06:53:32.612189Z","shell.execute_reply.started":"2024-05-27T06:53:31.770451Z","shell.execute_reply":"2024-05-27T06:53:32.611270Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Embedding, MultiHeadAttention, Dropout, LayerNormalization\nfrom tensorflow.keras.models import Model\nimport numpy as np\n\nclass TransformerBlock(tf.keras.layers.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n        super(TransformerBlock, self).__init__()\n        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.ffn = tf.keras.Sequential([\n            Dense(ff_dim, activation=\"relu\"), \n            Dense(embed_dim),\n        ])\n        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n        self.dropout1 = Dropout(rate)\n        self.dropout2 = Dropout(rate)\n\n    def call(self, inputs, training):\n        attn_output = self.att(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)\n\ndef create_model(vocab_size, embed_dim, num_heads, ff_dim, maxlen):\n    inputs = Input(shape=(maxlen,))\n    embedding_layer = Embedding(vocab_size, embed_dim)(inputs)\n    transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n    x = transformer_block(embedding_layer)\n    hidden = Dense(500)(x)\n    outputs = Dense(vocab_size)(hidden)\n    model = Model(inputs=inputs, outputs=outputs)\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-05-27T06:53:32.613330Z","iopub.execute_input":"2024-05-27T06:53:32.613588Z","iopub.status.idle":"2024-05-27T06:53:37.904678Z","shell.execute_reply.started":"2024-05-27T06:53:32.613566Z","shell.execute_reply":"2024-05-27T06:53:37.903698Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"2024-05-27 06:53:33.025556: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-27 06:53:33.025613: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-27 06:53:33.027135: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"new = pd.read_csv('/kaggle/input/3k-conversations-dataset-for-chatbot/Conversation.csv')","metadata":{"execution":{"iopub.status.busy":"2024-05-27T06:53:37.906235Z","iopub.execute_input":"2024-05-27T06:53:37.906749Z","iopub.status.idle":"2024-05-27T06:53:37.936344Z","shell.execute_reply.started":"2024-05-27T06:53:37.906722Z","shell.execute_reply":"2024-05-27T06:53:37.935470Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"tokenizer = tf.keras.preprocessing.text.Tokenizer()\ntokenizer.fit_on_texts(pd.concat([df, new.rename(columns = {'question': 'message'})]).message)\nvocab_size = len(tokenizer.word_index) + 1\nseqs = tokenizer.texts_to_sequences(df.message)\nmaxlen = 100\nseqs = tf.keras.preprocessing.sequence.pad_sequences(seqs, maxlen=maxlen, padding='post')\n\ndef mask_input(seqs, mask_prob=0.15):\n    random_masks = np.random.rand(*seqs.shape) < mask_prob\n    masked_seqs = np.where(random_masks, 32365, seqs) \n    return masked_seqs, seqs\n\nmasked_inputs, labels = mask_input(seqs)\nwith tf.device('/gpu:0'):\n    model = create_model(vocab_size, embed_dim=8, num_heads=2, ff_dim=8, maxlen=maxlen)\n    model.compile(optimizer=\"adam\", loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n    model.fit(masked_inputs, labels, epochs=3, batch_size=100)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T06:53:37.937564Z","iopub.execute_input":"2024-05-27T06:53:37.937841Z","iopub.status.idle":"2024-05-27T06:55:40.475766Z","shell.execute_reply.started":"2024-05-27T06:53:37.937818Z","shell.execute_reply":"2024-05-27T06:55:40.474858Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Epoch 1/3\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1716792824.045871     114 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"5000/5000 [==============================] - 53s 9ms/step - loss: 2.3677\nEpoch 2/3\n5000/5000 [==============================] - 34s 7ms/step - loss: 1.5261\nEpoch 3/3\n5000/5000 [==============================] - 34s 7ms/step - loss: 1.3544\n","output_type":"stream"}]},{"cell_type":"code","source":"w1 = model.layers[1].get_weights()\nw2 = model.layers[2].get_weights()\nw3 = model.layers[3].get_weights()\nw4 = model.layers[4].get_weights()","metadata":{"execution":{"iopub.status.busy":"2024-05-27T06:55:40.478582Z","iopub.execute_input":"2024-05-27T06:55:40.478889Z","iopub.status.idle":"2024-05-27T06:55:40.568378Z","shell.execute_reply.started":"2024-05-27T06:55:40.478862Z","shell.execute_reply":"2024-05-27T06:55:40.567367Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Embedding, MultiHeadAttention, Dropout, LayerNormalization, Flatten, Reshape\nfrom tensorflow.keras.models import Model\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-05-27T06:55:40.569552Z","iopub.execute_input":"2024-05-27T06:55:40.569836Z","iopub.status.idle":"2024-05-27T06:55:40.575611Z","shell.execute_reply.started":"2024-05-27T06:55:40.569813Z","shell.execute_reply":"2024-05-27T06:55:40.574514Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nprompts_seq = tokenizer.texts_to_sequences(new.question)\nanswers_seq = tokenizer.texts_to_sequences(new.answer)\n\nmaxlen_prompt = 20\nmaxlen_answer = 20\n\nprompts_seq = pad_sequences(prompts_seq, maxlen=maxlen_prompt, padding='post')\nanswers_seq = pad_sequences(answers_seq, maxlen=maxlen_answer, padding='post')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-27T07:00:46.018070Z","iopub.execute_input":"2024-05-27T07:00:46.018496Z","iopub.status.idle":"2024-05-27T07:00:46.145469Z","shell.execute_reply.started":"2024-05-27T07:00:46.018467Z","shell.execute_reply":"2024-05-27T07:00:46.144717Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Embedding, MultiHeadAttention, Dropout, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport numpy as np\n\nclass TransformerBlock(tf.keras.layers.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n        super(TransformerBlock, self).__init__()\n        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.ffn = tf.keras.Sequential([\n            Dense(ff_dim, activation=\"relu\"), \n            Dense(embed_dim),\n        ])\n        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n        self.dropout1 = Dropout(rate)\n        self.dropout2 = Dropout(rate)\n\n    def call(self, inputs, training):\n        attn_output = self.att(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)\n\ndef create_model(vocab_size, embed_dim, num_heads, ff_dim, maxlen_prompt, maxlen_answer):\n    inputs = Input(shape=(maxlen_prompt,))\n    embedding_layer = Embedding(vocab_size, embed_dim)(inputs)\n    transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n    x = transformer_block(embedding_layer, training=True)\n    x = Dense(500, activation='relu')(x)\n    outputs = Dense(vocab_size)(x)\n    outputs = tf.keras.layers.Reshape((maxlen_prompt, vocab_size))(outputs)\n    model = Model(inputs=inputs, outputs=outputs)\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2024-05-27T07:00:49.986783Z","iopub.execute_input":"2024-05-27T07:00:49.987159Z","iopub.status.idle":"2024-05-27T07:00:49.999532Z","shell.execute_reply.started":"2024-05-27T07:00:49.987130Z","shell.execute_reply":"2024-05-27T07:00:49.998412Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"model = create_model(vocab_size, embed_dim=8, num_heads=2, ff_dim=8, maxlen_prompt=maxlen_prompt, maxlen_answer=maxlen_answer)\n\nmodel.layers[1].set_weights(w1)\nmodel.layers[2].set_weights(w2)\nmodel.layers[3].set_weights(w3)\nmodel.layers[4].set_weights(w4)\n\nmodel.compile(optimizer=\"adam\", loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n\nlabels = np.expand_dims(answers_seq, axis=-1)\n\nwith tf.device('/gpu:0'):\n    model.fit(prompts_seq, labels, epochs=5, batch_size=1)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-27T07:00:52.233393Z","iopub.execute_input":"2024-05-27T07:00:52.234218Z","iopub.status.idle":"2024-05-27T07:02:32.444809Z","shell.execute_reply.started":"2024-05-27T07:00:52.234187Z","shell.execute_reply":"2024-05-27T07:02:32.443999Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Epoch 1/5\n3725/3725 [==============================] - 24s 6ms/step - loss: 2.5993\nEpoch 2/5\n3725/3725 [==============================] - 19s 5ms/step - loss: 2.3750\nEpoch 3/5\n3725/3725 [==============================] - 19s 5ms/step - loss: 2.3251\nEpoch 4/5\n3725/3725 [==============================] - 19s 5ms/step - loss: 2.2986\nEpoch 5/5\n3725/3725 [==============================] - 19s 5ms/step - loss: 2.2777\n","output_type":"stream"}]}]}